
# üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á](#–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ-–ø—Ä–∏–º–µ—Ä—ã-–∑–∞–¥–∞—á)
2. [–ú–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è](#–º–∞—Å—à—Ç–∞–±–Ω—ã–µ-—Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è)
3. [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏](#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
4. [–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏](#–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è-—Å-–≤–Ω–µ—à–Ω–∏–º–∏-—Å–∏—Å—Ç–µ–º–∞–º–∏)
5. [–ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ](#–∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è-–∏-—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ)
6. [–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞](#–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥-–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞)
7. [–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è](#—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ-–∏-–≤–∞–ª–∏–¥–∞—Ü–∏—è)

---

## üéØ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á

### –ü—Ä–∏–º–µ—Ä 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å MapReduce

```python
import asyncio
import sys
import os
import json
import random
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from main import ComputeNetwork
from core.task import Task, TaskType, TaskPriority

async def big_data_processing():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MapReduce"""
    
    print("üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å MapReduce")
    
    network = ComputeNetwork(host='127.0.0.1', port=5563)
    
    try:
        await network.start()
        await asyncio.sleep(2)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (1M –∑–∞–ø–∏—Å–µ–π)
        print("üì¶ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        large_dataset = []
        for i in range(1000000):
            record = {
                'id': i,
                'value': random.uniform(0, 100),
                'category': random.choice(['A', 'B', 'C', 'D']),
                'timestamp': time.time() - random.randint(0, 86400)  # –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
            }
            large_dataset.append(record)
        
        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(large_dataset)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ó–∞–¥–∞—á–∞ 1: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_task = Task.create_map_reduce(
            owner_id=network.node.node_id,
            data=large_dataset,
            map_function="record['category']",
            reduce_function="count_by_category",
            requirements={
                'cpu_percent': 60.0,
                'ram_gb': 4.0,
                'timeout_seconds': 600
            },
            config={
                'max_price': 5.0,
                'priority': TaskPriority.HIGH.value
            }
        )
        
        task_id1 = await network.submit_task(category_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å–æ–∑–¥–∞–Ω–∞: {task_id1}")
        
        # –ó–∞–¥–∞—á–∞ 2: –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º
        stats_task = Task.create_map_reduce(
            owner_id=network.node.node_id,
            data=large_dataset,
            map_function="record['value']",
            reduce_function="calculate_statistics",
            requirements={
                'cpu_percent': 50.0,
                'ram_gb': 3.0,
                'timeout_seconds': 300
            },
            config={
                'max_price': 3.0,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        task_id2 = await network.submit_task(stats_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–æ–∑–¥–∞–Ω–∞: {task_id2}")
        
        # –ó–∞–¥–∞—á–∞ 3: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        time_task = Task.create_map_reduce(
            owner_id=network.node.node_id,
            data=large_dataset,
            map_function="record['timestamp']",
            reduce_function="group_by_hour",
            requirements={
                'cpu_percent': 40.0,
                'ram_gb': 2.0,
                'timeout_seconds': 240
            },
            config={
                'max_price': 2.0,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        task_id3 = await network.submit_task(time_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∞: {task_id3}")
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        start_time = time.time()
        completed = 0
        
        while completed < 3:
            await asyncio.sleep(10)
            
            completed = 0
            for task_id in [task_id1, task_id2, task_id3]:
                status = await network.get_task_status(task_id)
                if status['status'] == 'completed':
                    completed += 1
                    print(f"‚úÖ –ó–∞–¥–∞—á–∞ {task_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
            elapsed = time.time() - start_time
            print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {completed}/3 –∑–∞–¥–∞—á –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {elapsed:.1f}s")
        
        # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = {}
        for task_id in [task_id1, task_id2, task_id3]:
            status = await network.get_task_status(task_id)
            if status['status'] == 'completed':
                results[task_id] = status.get('result', {})
        
        print(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        for task_id, result in results.items():
            print(f"  –ó–∞–¥–∞—á–∞ {task_id}: {result}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open('big_data_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
    finally:
        await network.stop()

# –ó–∞–ø—É—Å–∫
asyncio.run(big_data_processing())
```

### –ü—Ä–∏–º–µ—Ä 2: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π ML –ø–∞–π–ø–ª–∞–π–Ω

```python
import asyncio
import sys
import os
import numpy as np
import json
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from main import ComputeNetwork
from core.task import Task, TaskType, TaskPriority

async def ml_pipeline():
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π ML –ø–∞–π–ø–ª–∞–π–Ω —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —ç—Ç–∞–ø–∞–º–∏"""
    
    print("ü§ñ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π ML –ø–∞–π–ø–ª–∞–π–Ω")
    
    network = ComputeNetwork(host='127.0.0.1', port=5564)
    
    try:
        await network.start()
        await asyncio.sleep(2)
        
        # –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        def generate_synthetic_data():
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
            data = []
            for i in range(10000):
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                x1 = np.random.normal(0, 1)
                x2 = np.random.normal(0, 1)
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∫–∏ (–∫–ª–∞—Å—Å 0 –∏–ª–∏ 1)
                if x1 + x2 > 0:
                    label = 1
                else:
                    label = 0
                
                data.append([x1, x2, label])
            
            return data
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        data_task = Task.create_map(
            owner_id=network.node.node_id,
            data=list(range(10000)),
            function="generate_synthetic_data",
            requirements={
                'cpu_percent': 30.0,
                'ram_gb': 2.0
            },
            config={
                'max_price': 1.0,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        data_task_id = await network.submit_task(data_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞: {data_task_id}")
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        while True:
            status = await network.get_task_status(data_task_id)
            if status['status'] == 'completed':
                synthetic_data = status.get('result', {}).get('data', [])
                break
            await asyncio.sleep(2)
        
        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(synthetic_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
        
        # –≠—Ç–∞–ø 2: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/test
        print("üîÄ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/test...")
        
        split_task = Task.create_map_reduce(
            owner_id=network.node.node_id,
            data=synthetic_data,
            map_function="split_data",
            reduce_function="combine_splits",
            requirements={
                'cpu_percent': 20.0,
                'ram_gb': 1.0
            },
            config={
                'max_price': 0.5,
                'priority': TaskPriority.LOW.value
            }
        )
        
        split_task_id = await network.submit_task(split_task.to_dict())
        
        # –≠—Ç–∞–ø 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("üéì –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        train_task = Task.create_ml_train_step(
            owner_id=network.node.node_id,
            model_path="models/logistic_regression.pkl",
            train_data=synthetic_data[:8000],  # 80% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            model_type="sklearn",
            requirements={
                'cpu_percent': 70.0,
                'ram_gb': 3.0,
                'gpu_percent': 0.0
            },
            config={
                'max_price': 3.0,
                'priority': TaskPriority.HIGH.value
            }
        )
        
        train_task_id = await network.submit_task(train_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω–∞: {train_task_id}")
        
        # –≠—Ç–∞–ø 4: –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        print("üìà –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
        
        test_data = synthetic_data[8000:]  # 20% –¥–ª—è —Ç–µ—Å—Ç–∞
        
        eval_task = Task.create_ml_inference(
            owner_id=network.node.node_id,
            model_path="models/logistic_regression.pkl",
            input_data=test_data,
            model_type="sklearn",
            requirements={
                'cpu_percent': 40.0,
                'ram_gb': 2.0
            },
            config={
                'max_price': 1.0,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        eval_task_id = await network.submit_task(eval_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ –æ—Ü–µ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω–∞: {eval_task_id}")
        
        # –≠—Ç–∞–ø 5: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        def visualize_results(predictions, true_labels):
            """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
            import matplotlib.pyplot as plt
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            plt.figure(figsize=(10, 6))
            
            # ROC –∫—Ä–∏–≤–∞—è
            fpr, tpr, _ = roc_curve(true_labels, predictions)
            auc_score = auc(fpr, tpr)
            
            plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('ROC Curve')
            plt.legend()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            plt.savefig('roc_curve.png')
            plt.close()
            
            return {
                'auc_score': auc_score,
                'fpr': fpr.tolist(),
                'tpr': tpr.tolist()
            }
        
        vis_task = Task.create_map(
            owner_id=network.node.node_id,
            data=[predictions, true_labels],
            function="visualize_results",
            requirements={
                'cpu_percent': 30.0,
                'ram_gb': 1.0
            },
            config={
                'max_price': 0.5,
                'priority': TaskPriority.LOW.value
            }
        )
        
        vis_task_id = await network.submit_task(vis_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∞: {vis_task_id}")
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
        pipeline_tasks = [data_task_id, split_task_id, train_task_id, eval_task_id, vis_task_id]
        completed = 0
        
        while completed < len(pipeline_tasks):
            await asyncio.sleep(5)
            
            completed = 0
            for task_id in pipeline_tasks:
                status = await network.get_task_status(task_id)
                if status['status'] == 'completed':
                    completed += 1
                    print(f"‚úÖ –≠—Ç–∞–ø {pipeline_tasks.index(task_id) + 1} –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
        pipeline_results = {}
        for task_id in pipeline_tasks:
            status = await network.get_task_status(task_id)
            if status['status'] == 'completed':
                stage_name = ['data_generation', 'data_split', 'training', 'evaluation', 'visualization'][pipeline_tasks.index(task_id)]
                pipeline_results[stage_name] = status.get('result', {})
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞–π–ø–ª–∞–π–Ω–∞
        with open('ml_pipeline_results.json', 'w') as f:
            json.dump(pipeline_results, f, indent=2)
        
        print(f"üéâ ML –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        for stage, result in pipeline_results.items():
            print(f"  {stage}: {result}")
        
        # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏
        if 'evaluation' in pipeline_results:
            auc_score = pipeline_results['evaluation'].get('auc_score', 0)
            print(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ AUC: {auc_score:.3f}")
        
    finally:
        await network.stop()

# –ó–∞–ø—É—Å–∫
asyncio.run(ml_pipeline())
```

### –ü—Ä–∏–º–µ—Ä 3: –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```python
import asyncio
import sys
import os
import time
import json
import random
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from main import ComputeNetwork
from core.task import Task, TaskType, TaskPriority

class StreamProcessor:
    """–ü–æ—Ç–æ–∫–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, network: ComputeNetwork):
        self.network = network
        self.data_stream = []
        self.processing_tasks = []
        self.results = []
    
    async def generate_data_stream(self, duration_minutes=5):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        start_time = time.time()
        end_time = start_time + duration_minutes * 60
        
        while time.time() < end_time:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞–∫–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
            batch_size = random.randint(10, 50)
            batch = []
            
            for i in range(batch_size):
                data_point = {
                    'id': len(self.data_stream) + i,
                    'timestamp': time.time(),
                    'value': random.uniform(0, 100),
                    'category': random.choice(['A', 'B', 'C']),
                    'quality': random.choice(['high', 'medium', 'low'])
                }
                batch.append(data_point)
            
            self.data_stream.extend(batch)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞
            await self.process_batch(batch)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            await asyncio.sleep(random.uniform(0.1, 0.5))
        
        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(self.data_stream)} –∑–∞–ø–∏—Å–µ–π")
    
    async def process_batch(self, batch):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        # –ó–∞–¥–∞—á–∞ 1: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        agg_task = Task.create_map_reduce(
            owner_id=self.network.node.node_id,
            data=batch,
            map_function="record['category']",
            reduce_function="aggregate_by_category",
            requirements={
                'cpu_percent': 30.0,
                'ram_gb': 1.0
            },
            config={
                'max_price': 0.5,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        agg_task_id = await self.network.submit_task(agg_task.to_dict())
        
        # –ó–∞–¥–∞—á–∞ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        filter_task = Task.create_map(
            owner_id=self.network.node.node_id,
            data=batch,
            function="filter_by_quality",
            requirements={
                'cpu_percent': 20.0,
                'ram_gb': 0.5
            },
            config={
                'max_price': 0.2,
                'priority': TaskPriority.LOW.value
            }
        )
        
        filter_task_id = await self.network.submit_task(filter_task.to_dict())
        
        # –ó–∞–¥–∞—á–∞ 3: –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats_task = Task.create_map_reduce(
            owner_id=self.network.node.node_id,
            data=batch,
            map_function="record['value']",
            reduce_function="calculate_statistics",
            requirements={
                'cpu_percent': 25.0,
                'ram_gb': 0.8
            },
            config={
                'max_price': 0.3,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        stats_task_id = await self.network.submit_task(stats_task.to_dict())
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ID –∑–∞–¥–∞—á
        self.processing_tasks.extend([
            (agg_task_id, 'aggregation'),
            (filter_task_id, 'filter'),
            (stats_task_id, 'statistics')
        ])
    
    async def monitor_progress(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        print("üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞...")
        
        processed_count = 0
        total_tasks = len(self.processing_tasks)
        
        while processed_count < total_tasks:
            await asyncio.sleep(2)
            
            completed = 0
            for task_id, task_type in self.processing_tasks:
                status = await self.network.get_task_status(task_id)
                if status['status'] == 'completed':
                    completed += 1
                    self.results.append({
                        'task_id': task_id,
                        'type': task_type,
                        'result': status.get('result', {}),
                        'timestamp': time.time()
                    })
                    print(f"‚úÖ {task_type} –∑–∞–¥–∞—á–∞ {task_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
            processed_count = completed
            progress = (processed_count / total_tasks) * 100
            
            print(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed_count}/{total_tasks} ({progress:.1f}%)")
        
        print("üéâ –í—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    
    async def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞"""
        
        print("üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        report = {
            'total_records': len(self.data_stream),
            'processing_time': time.time() - self.results[0]['timestamp'] if self.results else 0,
            'tasks_completed': len(self.results),
            'results_by_type': {},
            'summary_statistics': {}
        }
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á
        for result in self.results:
            task_type = result['type']
            if task_type not in report['results_by_type']:
                report['results_by_type'][task_type] = []
            report['results_by_type'][task_type].append(result['result'])
        
        # –†–∞—Å—á–µ—Ç —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if 'statistics' in report['results_by_type']:
            all_stats = []
            for stat_result in report['results_by_type']['statistics']:
                if isinstance(stat_result, dict):
                    all_stats.extend([
                        stat_result.get('mean', 0),
                        stat_result.get('median', 0),
                        stat_result.get('std', 0)
                    ])
            
            if all_stats:
                report['summary_statistics'] = {
                    'mean_value': sum(all_stats) / len(all_stats),
                    'total_values': len(all_stats)
                }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        with open('stream_processing_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"üìä –û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: stream_processing_report.json")
        print(f"üìà –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {report['total_records']}")
        print(f"‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–¥–∞—á: {report['tasks_completed']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {report['processing_time']:.2f}—Å")
        
        return report

async def stream_processing_example():
    """–ü—Ä–∏–º–µ—Ä –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    print("üåä –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    
    network = ComputeNetwork(host='127.0.0.1', port=5565)
    
    try:
        await network.start()
        await asyncio.sleep(2)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
        processor = StreamProcessor(network)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        generate_task = asyncio.create_task(processor.generate_data_stream(duration_minutes=2))
        monitor_task = asyncio.create_task(processor.monitor_progress())
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        await generate_task
        await monitor_task
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = await processor.generate_report()
        
        return report
        
    finally:
        await network.stop()

# –ó–∞–ø—É—Å–∫
asyncio.run(stream_processing_example())
```

### –ü—Ä–∏–º–µ—Ä 4: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

```python
import asyncio
import sys
import os
import time
import json
import random
from PIL import Image
import numpy as np
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from main import ComputeNetwork
from core.task import Task, TaskType, TaskPriority

async def image_processing_pipeline():
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    print("üñºÔ∏è –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    network = ComputeNetwork(host='127.0.0.1', port=5566)
    
    try:
        await network.start()
        await asyncio.sleep(2)
        
        # –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        print("üì∏ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        def generate_test_images(count=10):
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
            images = []
            
            for i in range(count):
                # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                width = random.randint(256, 512)
                height = random.randint(256, 512)
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                img_array = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)
                img = Image.fromarray(img_array)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                filename = f"test_image_{i}.png"
                img.save(filename)
                
                images.append({
                    'filename': filename,
                    'size': f"{width}x{height}",
                    'format': 'PNG'
                })
            
            return images
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        generate_task = Task.create_map(
            owner_id=network.node.node_id,
            data=list(range(10)),
            function="generate_test_images",
            requirements={
                'cpu_percent': 40.0,
                'ram_gb': 2.0
            },
            config={
                'max_price': 2.0,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        generate_task_id = await network.submit_task(generate_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞: {generate_task_id}")
        
        # –≠—Ç–∞–ø 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        print("üîÑ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        processing_tasks = []
        
        for i in range(10):
            # –ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            process_task = Task.create_matrix_ops(
                owner_id=network.node.node_id,
                matrix1=[[i, i+1], [i+2, i+3]],  # –ò–º–∏—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                operation="apply_filter",
                requirements={
                    'cpu_percent': 60.0,
                    'ram_gb': 1.0,
                    'gpu_percent': 50.0
                },
                config={
                    'max_price': 1.0,
                    'priority': TaskPriority.HIGH.value
                }
            )
            
            task_id = await network.submit_task(process_task.to_dict())
            processing_tasks.append((task_id, f"image_{i}"))
            print(f"‚úÖ –ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i} —Å–æ–∑–¥–∞–Ω–∞: {task_id}")
        
        # –≠—Ç–∞–ø 3: –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("üìä –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        aggregate_task = Task.create_map_reduce(
            owner_id=network.node.node_id,
            data=[result for _, result in processing_tasks],
            map_function="extract_features",
            reduce_function="combine_features",
            requirements={
                'cpu_percent': 30.0,
                'ram_gb': 1.0
            },
            config={
                'max_price': 1.0,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        aggregate_task_id = await network.submit_task(aggregate_task.to_dict())
        print(f"‚úÖ –ó–∞–¥–∞—á–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∞: {aggregate_task_id}")
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        all_tasks = [generate_task_id] + [task_id for task_id, _ in processing_tasks] + [aggregate_task_id]
        completed = 0
        
        while completed < len(all_tasks):
            await asyncio.sleep(3)
            
            completed = 0
            for task_id in all_tasks:
                status = await network.get_task_status(task_id)
                if status['status'] == 'completed':
                    completed += 1
                    task_name = {
                        generate_task_id: "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                        aggregate_task_id: "–ê–≥—Ä–µ–≥–∞—Ü–∏—è"
                    }.get(task_id, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                    
                    print(f"‚úÖ {task_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = {}
        for task_id in all_tasks:
            status = await network.get_task_status(task_id)
            if status['status'] == 'completed':
                results[task_id] = status.get('result', {})
        
        # –≠—Ç–∞–ø 4: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–∞–∂–∞
        print("üé® –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–∞–∂–∞...")
        
        collage_task = Task.create_matrix_ops(
            owner_id=network.node.node_id,
            matrix1=[[1, 2, 3], [4, 5, 6], [7, 8, 9]],  # –ò–º–∏—Ç–∞—Ü–∏—è –∫–æ–ª–ª–∞–∂–∞
            operation="create_collage",
            requirements={
                'cpu_percent': 50.0,
                'ram_gb': 2.0
            },
            config={
                'max_price': 1.5,
                'priority': TaskPriority.NORMAL.value
            }
        )
        
        collage_task_id = await network.submit_task(collage_task.to_dict())
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–æ–ª–ª–∞–∂–∞
        while True:
            status = await network.get_task_status(collage_task_id)
            if status['status'] == 'completed':
                break
            await asyncio.sleep(2)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        print("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        
        final_report = {
            'total_images': 10,
            'processing_time': time.time() - start_time,
            'tasks_completed': len(all_tasks),
            'results': results,
            'collage_created': True,
            'timestamp': time.time()
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        with open('image_processing_report.json', 'w') as f:
            json.dump(final_report, f, indent=2)
        
        print(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –û—Ç—á–µ—Ç: image_processing_report.json")
        
        return final_report
        
    finally:
        await network.stop()

# –ó–∞–ø—É—Å–∫
asyncio.run(image_processing_pipeline())
```

---

## üöÄ –ú–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

### Docker Compose —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ

#### 1. –°–æ–∑–¥–∞–Ω–∏–µ docker-compose.yml

```yaml
version: '3.8'

services:
  # –°–µ—Ä–≤–∏—Å –¥–ª—è —Å–µ—Ç–∏
  compute-network:
    build: .
    ports:
      - "5555:5555"
      - "5556:5556"
      - "5557:5557"
    environment:
      - NODE_ID=node1
      - PORT=5555
      - DEBUG=true
    volumes:
      - ./config:/app/config
      - ./logs:/app/logs
    networks:
      - compute-net
    restart: unless-stopped

  # –í—Ç–æ—Ä–æ–π —É–∑–µ–ª
  compute-network-2:
    build: .
    ports:
      - "5558:5555"
    environment:
      - NODE_ID=node2
      - PORT=5555
      - DEBUG=true
    volumes:
      - ./config:/app/config
      - ./logs:/app/logs
    networks:
      - compute-net
    restart: unless-stopped
    depends_on:
      - compute-network

  # –¢—Ä–µ—Ç–∏–π —É–∑–µ–ª
  compute-network-3:
    build: .
    ports:
      - "5559:5555"
    environment:
      - NODE_ID=node3
      - PORT=5555
      - DEBUG=true
    volumes:
      - ./config:/app/config
      - ./logs:/app/logs
    networks:
      - compute-net
    restart: unless-stopped
    depends_on:
      - compute-network

  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
  monitoring:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - compute-net

  # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./monitoring/grafana:/etc/grafana/provisioning
    networks:
      - compute-net

networks:
  compute-net:
    driver: bridge

volumes:
  config:
  logs:
```

#### 2. Dockerfile

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞
COPY src/ ./src/
COPY config/ ./config/
COPY examples/ ./examples/

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
RUN mkdir -p /app/logs /app/data

# –û—Ç–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ä—Ç—ã
EXPOSE 5555

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
ENV PYTHONPATH=/app/src
ENV NODE_ID=node1
ENV PORT=5555
ENV DEBUG=false

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
CMD ["python", "src/main.py", "--host", "0.0.0.0", "--port", "5555"]
```

#### 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Prometheus

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'compute-network'
    static_configs:
      - targets: ['compute-network:5555', 'compute-network-2:5555', 'compute-network-3:5555']
    metrics_path: /metrics
    scrape_interval: 10s
```

#### 4. –ó–∞–ø—É—Å–∫ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

```bash
# –°–±–æ—Ä–∫–∞ –∏ –∑–∞–ø—É—Å–∫
docker-compose up -d

# –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç—É—Å–∞
docker-compose ps

# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
docker-compose logs -f compute-network

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
docker-compose up -d --scale compute-network=5

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞
docker-compose down

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
docker-compose pull
docker-compose up -d --force-recreate
```

### Kubernetes —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ

#### 1. Helm Chart

```yaml
# charts/compute-network/values.yaml
replicaCount: 3

image:
  repository: compute-network
  tag: latest
  pullPolicy: IfNotPresent

service:
  type: LoadBalancer
  port: 80
  targetPort: 5555

resources:
  limits:
    cpu: 1
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 512Mi

config:
  nodeType: "worker"
  maxPeers: 100
  debug: false

monitoring:
  enabled: true
  prometheusPort: 9090
```

#### 2. Deployment manifest

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: compute-network
  labels:
    app: compute-network
spec:
  replicas: 3
  selector:
    matchLabels:
      app: compute-network
  template:
    metadata:
      labels:
        app: compute-network
    spec:
      containers:
      - name: compute-network
        image: compute-network:latest
        ports:
        - containerPort: 5555
        env:
        - name: NODE_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        volumeMounts:
        - name: config
          mountPath: /app/config
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: config
        configMap:
          name: compute-network-config
      - name: logs
        emptyDir: {}
```

#### 3. Service manifest

```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: compute-network-service
spec:
  selector:
    app: compute-network
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5555
  type: LoadBalancer
```

#### 4. ConfigMap

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: compute-network-config
data:
  network.json: |
    {
      "discovery_interval": 30,
      "max_peers": 100,
      "timeout": 60,
      "retry_attempts": 3
    }
  pricing.json: |
    {
      "base_cpu_price": 0.01,
      "base_gpu_price": 0.05,
      "urgency_multiplier": {
        "low": 0.8,
        "normal": 1.0,
        "high": 1.5
      }
    }
```

#### 5. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–º

```bash
# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
kubectl apply -f k8s/

# –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç—É—Å–∞
kubectl get pods
kubectl get services
kubectl get deployments

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
kubectl scale deployment compute-network --replicas=5

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
kubectl set image deployment/compute-network compute-network=compute-network:v2.0

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
kubectl logs -f deployment/compute-network
kubectl top pods

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞
kubectl delete -f k8s/
```

### –û–±–ª–∞—á–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ AWS

#### 1. Terraform –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```hcl
# main.tf
provider "aws" {
  region = "us-east-1"
}

# VPC
resource "aws_vpc" "compute_vpc" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_support   = true
  enable_dns_hostnames = true
  
  tags = {
    Name = "compute-network-vpc"
  }
}

# Subnets
resource "aws_subnet" "compute_subnet" {
  vpc_id                  = aws_vpc.compute_vpc.id
  cidr_block              = "10.0.1.0/24"
  map_public_ip_on_launch = true
  
  tags = {
    Name = "compute-network-subnet"
  }
}

# Security Group
resource "aws_security_group" "compute_sg" {
  vpc_id = aws_vpc.compute_vpc.id
  
  ingress {
    from_port   = 5555
    to_port     = 5555
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  tags = {
    Name = "compute-network-sg"
  }
}

# Auto Scaling Group
resource "aws_launch_configuration" "compute_lc" {
  name_prefix   = "compute-node-"
  image_id      = "ami-0c55b159cbfafe1f0"
  instance_type = "t3.medium"
  key_name      = "compute-key"
  
  security_groups = [aws_security_group.compute_sg.id]
  
  user_data = <<-EOF
    #!/bin/bash
    apt-get update
    apt-get install -y python3 python3-pip
    pip3 install -r /tmp/requirements.txt
    
    # –ó–∞–ø—É—Å–∫ compute node
    python3 /tmp/src/main.py --host 0.0.0.0 --port 5555 --debug
  EOF
  
  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_autoscaling_group" "compute_asg" {
  desired_capacity    = 3
  max_size           = 10
  min_size           = 2
  vpc_zone_identifier = [aws_subnet.compute_subnet.id]
  
  launch_configuration = aws_launch_configuration.compute_lc.name
  
  tag {
    key                 = "Name"
    value               = "compute-node"
    propagate_at_launch = true
  }
}

# Load Balancer
resource "aws_elb" "compute_elb" {
  name               = "compute-network-elb"
  availability_zones = ["us-east-1a"]
  
  listener {
    instance_port     = 5555
    instance_protocol = "tcp"
    lb_port           = 80
    lb_protocol       = "tcp"
  }
  
  health_check {
    healthy_threshold   = 2
    unhealthy_threshold = 2
    timeout             = 3
    target              = "TCP:5555"
    interval            = 30
  }
  
  instances = aws_autoscaling_group.compute_asg.ids
  
  cross_zone_load_balancing   = true
  connection_draining         = true
  connection_draining_timeout = 400
  
  tags = {
    Name = "compute-network-elb"
  }
}
```

#### 2. Ansible playbook

```yaml
# ansible/playbook.yml
---
- name: Deploy Compute Network
  hosts: compute_nodes
  become: yes
  
  vars:
    python_version: "3.9"
    node_port: 5555
    debug_mode: true
  
  tasks:
    - name: Install Python dependencies
      apt:
        name: "{{ item }}"
        state: present
      loop:
        - python3
        - python3-pip
        - build-essential
    
    - name: Create application directory
      file:
        path: /opt/compute-network
        state: directory
        mode: '0755'
    
    - name: Copy application files
      copy:
        src: "{{ playbook_dir }}/"
        dest: /opt/compute-network/
        mode: '0644'
    
    - name: Install Python packages
      pip:
        requirements: /opt/compute-network/requirements.txt
        virtualenv: /opt/compute-network/venv
    
    - name: Create systemd service
      template:
        src: templates/compute-network.service.j2
        dest: /etc/systemd/system/compute-network.service
        mode: '0644'
    
    - name: Enable and start service
      systemd:
        name: compute-network
        state: started
        enabled: yes
        daemon_reload: yes
    
    - name: Configure firewall
      ufw:
        rule: allow
        port: "{{ node_port }}"
        proto: tcp
```

---

## ‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ö–ª–∞—Å—Ç–µ—Ä–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

#### 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏

```python
import asyncio
import time
from typing import Dict, List
from dataclasses import dataclass

@dataclass
class LoadBalancer:
    """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫ –Ω–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    
    def __init__(self, network):
        self.network = network
        self.node_loads: Dict[str, float] = {}
        self.task_queue = asyncio.Queue()
        self.running = False
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫–∞"""
        self.running = True
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏
        asyncio.create_task(self.monitor_load())
        asyncio.create_task(self.distribute_tasks())
        
        print("üîÑ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫ –Ω–∞–≥—Ä—É–∑–∫–∏ –∑–∞–ø—É—â–µ–Ω")
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫–∞"""
        self.running = False
    
    async def monitor_load(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —É–∑–ª—ã"""
        while self.running:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Ç–∏
                status = await self.network.get_network_status()
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —É–∑–ª–∞—Ö
                for peer_id in self.network.node.peers:
                    # –ò–º–∏—Ç–∞—Ü–∏—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
                    load = await self.get_node_load(peer_id)
                    self.node_loads[peer_id] = load
                
                # –í—ã–≤–æ–¥ —Ç–µ–∫—É—â–µ–π –Ω–∞–≥—Ä—É–∑–∫–∏
                print(f"üìä –¢–µ–∫—É—â–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞: {self.node_loads}")
                
                await asyncio.sleep(10)  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–∞–≥—Ä—É–∑–∫–∏: {e}")
                await asyncio.sleep(30)
    
    async def get_node_load(self, node_id: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —É–∑–ª–µ"""
        try:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π —Å–±–æ—Ä –º–µ—Ç—Ä–∏–∫
            # –ù–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ API —É–∑–ª–∞ –∏–ª–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É
            
            # –ò–º–∏—Ç–∞—Ü–∏—è: —Å–ª—É—á–∞–π–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
            import random
            return random.uniform(0.1, 0.9)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ —É–∑–ª–∞ {node_id}: {e}")
            return 1.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    async def distribute_tasks(self):
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á –ø–æ —É–∑–ª–∞–º"""
        while self.running:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                task_data = await asyncio.wait_for(self.task_queue.get(), timeout=1.0)
                
                # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —É–∑–µ–ª
                optimal_node = self.find_optimal_node(task_data)
                
                if optimal_node:
                    # –ù–∞–∑–Ω–∞—á–∞–µ–º –∑–∞–¥–∞—á—É
                    await self.assign_task(optimal_node, task_data)
                else:
                    # –ó–∞–¥–∞—á—É –Ω–µ–ª—å–∑—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å
                    print(f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —É–∑–ª–æ–≤ –¥–ª—è –∑–∞–¥–∞—á–∏ {task_data.get('task_id')}")
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–¥–∞—á: {e}")
    
    def find_optimal_node(self, task_data: Dict) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —É–∑–µ–ª –¥–ª—è –∑–∞–¥–∞—á–∏"""
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —É–∑–ª—ã
        available_nodes = [
            node_id for node_id, load in self.node_loads.items()
            if load < 0.8  # –ù–∞–≥—Ä—É–∑–∫–∞ –º–µ–Ω–µ–µ 80%
        ]
        
        if not available_nodes:
            return None
        
        # –í—ã–±–∏—Ä–∞–µ–º —É–∑–µ–ª —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π
        optimal_node = min(available_nodes, key=lambda x: self.node_loads[x])
        
        return optimal_node
    
    async def assign_task(self, node_id: str, task_data: Dict):
        """–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ —É–∑–ª—É"""
        try:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
            print(f"üìù –ó–∞–¥–∞—á–∞ {task_data.get('task_id')} –Ω–∞–∑–Ω–∞—á–µ–Ω–∞ —É–∑–ª—É {node_id}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–≥—Ä—É–∑–∫—É
            self.node_loads[node_id] += 0.1
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–¥–∞—á–∏: {e}")
    
    async def submit_task(self, task_data: Dict):
        """–ü–æ–¥–∞—á–∞ –∑–∞–¥–∞—á–∏ –≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫"""
        await self.task_queue.put(task_data)
        print(f"üì§ –ó–∞–¥–∞—á–∞ {task_data.get('task_id')} –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å")
```

#### 2. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
import asyncio
import time
import hashlib
from typing import Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class CacheEntry:
    """–ó–∞–ø–∏—Å—å –≤ –∫—ç—à–µ"""
    data: Any
    timestamp: float
    ttl: float
    access_count: int = 0
    
    def is_expired(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–µ—á–µ–Ω–∏—è —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è"""
        return time.time() - self.timestamp > self.ttl
    
    def touch(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ—Å—Ç—É–ø–∞"""
        self.access_count += 1
        self.timestamp = time.time()

class TaskCache:
    """–ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–¥–∞—á"""
    
    def __init__(self, max_size: int = 1000, default_ttl: float = 3600):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.cache: Dict[str, CacheEntry] = {}
        self.access_times: Dict[str, float] = {}
    
    def _generate_key(self, task_data: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –¥–ª—è –∑–∞–¥–∞—á–∏"""
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–¥–∞—á–∏
        key_data = {
            'task_type': task_data.get('task_type'),
            'parameters': task_data.get('parameters', {}),
            'requirements': task_data.get('requirements', {})
        }
        
        key_str = str(key_data)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _evict_expired(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π"""
        current_time = time.time()
        
        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        expired_keys = [
            key for key, entry in self.cache.items()
            if entry.is_expired()
        ]
        
        for key in expired_keys:
            del self.cache[key]
            print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å: {key}")
    
    def _evict_lru(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        if len(self.cache) <= self.max_size:
            return
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∑–∞–ø–∏—Å–∏
        sorted_keys = sorted(
            self.cache.keys(),
            key=lambda k: self.access_times.get(k, 0)
        )
        
        # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
        keys_to_remove = sorted_keys[:len(self.cache) - self.max_size]
        
        for key in keys_to_remove:
            del self.cache[key]
            print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ LRU –∑–∞–ø–∏—Å—å: {key}")
    
    async def get(self, task_data: Dict) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –∫—ç—à–∞"""
        key = self._generate_key(task_data)
        
        if key in self.cache:
            entry = self.cache[key]
            
            if entry.is_expired():
                del self.cache[key]
                return None
            
            entry.touch()
            self.access_times[key] = time.time()
            
            print(f"üéØ –•–∏—Ç –∫—ç—à–∞ –¥–ª—è –∑–∞–¥–∞—á–∏: {key}")
            return entry.data
        
        return None
    
    async def set(self, task_data: Dict, result: Any, ttl: Optional[float] = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫—ç—à"""
        key = self._generate_key(task_data)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º TTL
        if ttl is None:
            ttl = self.default_ttl
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å
        entry = CacheEntry(
            data=result,
            timestamp=time.time(),
            ttl=ttl
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
        if len(self.cache) >= self.max_size:
            self._evict_expired()
            self._evict_lru()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å—å
        self.cache[key] = entry
        self.access_times[key] = time.time()
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫—ç—à: {key}")
    
    async def clear(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        self.cache.clear()
        self.access_times.clear()
        print("üßπ –ö—ç—à –æ—á–∏—â–µ–Ω")
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
        return {
            'total_entries': len(self.cache),
            'max_size': self.max_size,
            'hit_rate': self._calculate_hit_rate(),
            'avg_access_time': self._calculate_avg_access_time()
        }
    
    def _calculate_hit_rate(self) -> float:
        """–†–∞—Å—á–µ—Ç —Ö–∏—Ç-—Ä–µ–π—Ç–∞"""
        if not self.access_times:
            return 0.0
        
        total_accesses = sum(self.access_times.values())
        if total_accesses == 0:
            return 0.0
        
        hit_count = len([t for t in self.access_times.values() if t > 0])
        return hit_count / len(self.access_times)
    
    def _calculate_avg_access_time(self) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ—Å—Ç—É–ø–∞"""
        if not self.access_times:
            return 0.0
        
        return sum(self.access_times.values()) / len(self.access_times)

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–µ—Ç—å—é
class CachedNetwork:
    """–°–µ—Ç—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, network, cache: TaskCache):
        self.network = network
        self.cache = cache
    
    async def submit_task(self, task_data: Dict) -> str:
        """–ü–æ–¥–∞—á–∞ –∑–∞–¥–∞—á–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫—ç—à–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤ –∫—ç—à–µ
        cached_result = await self.cache.get(task_data)
        
        if cached_result is not None:
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º ID –∑–∞–¥–∞—á–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
            return f"cached_{hash(str(task_data))}"
        
        # –ü–æ–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ —Å–µ—Ç—å
        task_id = await self.network.submit_task(task_data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à–µ –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        return task_id
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏

#### 1. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é

```python
import asyncio
import psutil
import gc
from typing import Dict, List
from dataclasses import dataclass

@dataclass
class MemoryManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, max_memory_percent: float = 80.0):
        self.max_memory_percent = max_memory_percent
        self.memory_history: List[float] = []
        self.running = False
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏"""
        self.running = True
        
        asyncio.create_task(self.monitor_memory())
        asyncio.create_task(self.optimize_memory())
        
        print("üß† –ú–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏ –∑–∞–ø—É—â–µ–Ω")
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–∞–º—è—Ç–∏"""
        self.running = False
    
    async def monitor_memory(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        while self.running:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏
                memory = psutil.virtual_memory()
                memory_percent = memory.percent
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
                self.memory_history.append(memory_percent)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
                if len(self.memory_history) > 100:
                    self.memory_history.pop(0)
                
                # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                print(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_percent:.1f}%")
                
                await asyncio.sleep(30)  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏: {e}")
                await asyncio.sleep(60)
    
    async def optimize_memory(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        while self.running:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
                memory = psutil.virtual_memory()
                memory_percent = memory.percent
                
                if memory_percent > self.max_memory_percent:
                    print(f"‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_percent:.1f}%")
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
                    await self.perform_memory_optimization()
                
                await asyncio.sleep(60)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 60 —Å–µ–∫—É–Ω–¥
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: {e}")
                await asyncio.sleep(120)
    
    async def perform_memory_optimization(self):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏"""
        print("üîß –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏...")
        
        try:
            # 1. –°–±–æ—Ä –º—É—Å–æ—Ä–∞
            collected = gc.collect()
            print(f"üóëÔ∏è –°–æ–±—Ä–∞–Ω–æ –º—É—Å–æ—Ä–∞: {collected} –æ–±—ä–µ–∫—Ç–æ–≤")
            
            # 2. –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if hasattr(self, 'cache'):
                await self.cache.clear()
            
            # 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏
            await self.optimize_data_structures()
            
            # 4. –°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö
            await self.compress_data()
            
            print("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: {e}")
    
    async def optimize_data_structures(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö
        # –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–∂–∞—Ç–∏–µ —Å–ø–∏—Å–∫–æ–≤, —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —Ç.–¥.
        
        print("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü—Ä–∏–º–µ—Ä: –æ—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–æ–≤
        if hasattr(self, 'large_lists'):
            for name, lst in self.large_lists.items():
                if len(lst) > 10000:
                    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    lst = [item for item in lst if time.time() - item.get('timestamp', 0) < 3600]
                    self.large_lists[name] = lst
                    print(f"üßπ –û—á–∏—â–µ–Ω —Å–ø–∏—Å–æ–∫ {name}: {len(lst)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    async def compress_data(self):
        """–°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏"""
        print("üóúÔ∏è –°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö
        # –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–∂–∞—Ç–∏–µ —Å—Ç—Ä–æ–∫, —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ç.–¥.
        
        # –ü—Ä–∏–º–µ—Ä: —Å–∂–∞—Ç–∏–µ —Å—Ç—Ä–æ–∫
        if hasattr(self, 'string_data'):
            for key, data in self.string_data.items():
                if isinstance(data, str) and len(data) > 1000:
                    # –ü—Ä–æ—Å—Ç–æ–µ —Å–∂–∞—Ç–∏–µ (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å zlib/gzip)
                    compressed = data[:100] + "...[—Å–∂–∞—Ç–æ]..."
                    self.string_data[key] = compressed
                    print(f"üóúÔ∏è –°–∂–∞—Ç–∞ —Å—Ç—Ä–æ–∫–∞ {key}: {len(data)} -> {len(compressed)} —Å–∏–º–≤–æ–ª–æ–≤")
```

---

## üîå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏

### REST API —Å–µ—Ä–≤–µ—Ä

#### 1. FastAPI —Å–µ—Ä–≤–µ—Ä

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional, Any
import asyncio
import json
import time
from contextlib import asynccontextmanager

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à—É —Å–µ—Ç—å
from main import ComputeNetwork
from core.task import Task, TaskType, TaskPriority

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å–µ—Ç–∏
network = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    global network
    network = ComputeNetwork(host='0.0.0.0', port=5555)
    await network.start()
    print("üöÄ Compute Network API –∑–∞–ø—É—â–µ–Ω")
    
    yield
    
    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    await network.stop()
    print("üõë Compute Network API –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="Compute Network API",
    description="API –¥–ª—è –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–µ—Ç–∏",
    version="1.0.0",
    lifespan=lifespan
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic –º–æ–¥–µ–ª–∏
class TaskRequest(BaseModel):
    task_type: str
    owner_id: str
    parameters: Dict[str, Any]
    requirements: Dict[str, float]
    config: Dict[str, Any]

class TaskResponse(BaseModel):
    task_id: str
    status: str
    message: str

class NetworkStatus(BaseModel):
    node_id: str
    host: str
    port: int
    peers_count: int
    active_tasks: int
    credits: float
    reputation_score: float

class BatchRequest(BaseModel):
    tasks: List[TaskRequest]

# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "message": "Compute Network API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è API"""
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "network_running": network is not None
    }

@app.post("/api/v1/tasks", response_model=TaskResponse)
async def submit_task(task_request: TaskRequest):
    """–ü–æ–¥–∞—á–∞ –∑–∞–¥–∞—á–∏"""
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –≤ –∑–∞–¥–∞—á—É
        task = Task.create_task_from_request(task_request)
        
        # –ü–æ–¥–∞—á–∞ –≤ —Å–µ—Ç—å
        task_id = await network.submit_task(task.to_dict())
        
        return TaskResponse(
            task_id=task_id,
            status="pending",
            message="Task submitted successfully"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/api/v1/tasks/{task_id}")
async def get_task_status(task_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏"""
    try:
        status = await network.get_task_status(task_id)
        return status
        
    except Exception as e:
        raise HTTPException(status_code=404, detail="Task not found")

@app.get("/api/v1/network/status")
async def get_network_status():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ç–∏"""
    try:
        status = await network.get_network_status()
        return status
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/tasks/batch", response_model=List[TaskResponse])
async def submit_batch_tasks(batch_request: BatchRequest):
    """–ü–∞–∫–µ—Ç–Ω–∞—è –ø–æ–¥–∞—á–∞ –∑–∞–¥–∞—á"""
    responses = []
    
    for task_request in batch_request.tasks:
        try:
            task = Task.create_task_from_request(task_request)
            task_id = await network.submit_task(task.to_dict())
            
            responses.append(TaskResponse(
                task_id=task_id,
                status="pending",
                message="Task submitted successfully"
            ))
            
        except Exception as e:
            responses.append(TaskResponse(
                task_id="",
                status="error",
                message=str(e)
            ))
    
    return responses

@app.get("/api/v1/network/metrics")
async def get_network_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ç–∏"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        status = await network.get_network_status()
        pricing_analytics = network.pricing_engine.get_pricing_analytics()
        credit_health = network.credit_manager.get_network_health()
        
        return {
            "network_status": status,
            "pricing_analytics": pricing_analytics,
            "credit_health": credit_health,
            "timestamp": time.time()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/reputation/{node_id}")
async def get_node_reputation(node_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ —É–∑–ª–∞"""
    try:
        score = await network.reputation_manager.get_reputation_score(node_id)
        level = await network.reputation_manager.get_reputation_level(node_id)
        
        return {
            "node_id": node_id,
            "reputation_score": score,
            "reputation_level": level,
            "timestamp": time.time()
        }
        
    except Exception as e:
        raise HTTPException(status_code=404, detail="Node not found")

@app.post("/api/v1/tasks/{task_id}/cancel")
async def cancel_task(task_id: str, reason: str):
    """–û—Ç–º–µ–Ω–∞ –∑–∞–¥–∞—á–∏"""
    try:
        await network.cancel_task(task_id, reason)
        return {"message": f"Task {task_id} cancelled successfully"}
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/api/v1/types")
async def get_task_types():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á"""
    return {
        "task_types": [t.value for t in TaskType],
        "priorities": [p.value for p in TaskPriority]
    }

@app.get("/api/v1/examples")
async def get_task_examples():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∑–∞–¥–∞—á"""
    return {
        "examples": {
            "range_reduce": {
                "description": "–û–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º —á–∏—Å–µ–ª",
                "parameters": {
                    "start": "–ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
                    "end": "–ö–æ–Ω–µ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
                    "operation": "–û–ø–µ—Ä–∞—Ü–∏—è (sum, product, min, max, average)"
                }
            },
            "map": {
                "description": "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É",
                "parameters": {
                    "data": "–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö",
                    "function": "–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"
                }
            },
            "ml_inference": {
                "description": "–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π",
                "parameters": {
                    "model_path": "–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏",
                    "input_data": "–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ",
                    "model_type": "–¢–∏–ø –º–æ–¥–µ–ª–∏ (pytorch, tensorflow)"
                }
            }
        }
    }

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### 2. GraphQL API

```python
from fastapi import FastAPI
from fastapi.graphql import GraphQLApp
from graphql import (
    GraphQLSchema,
    GraphQLObjectType,
    GraphQLField,
    GraphQLString,
    GraphQLList,
    GraphQLNonNull,
    GraphQLArgument,
    GraphQLResolveInfo,
    graphql_sync
)
import json

# GraphQL —Å—Ö–µ–º–∞
TaskType = GraphQLObjectType(
    name="Task",
    fields={
        "task_id": GraphQLField(GraphQLNonNull(GraphQLString)),
        "status": GraphQLField(GraphQLString),
        "owner_id": GraphQLField(GraphQLString),
        "task_type": GraphQLField(GraphQLString),
        "result": GraphQLField(GraphQLString),
        "created_at": GraphQLField(GraphQLString),
        "updated_at": GraphQLField(GraphQLString),
    }
)

NetworkStatusType = GraphQLObjectType(
    name="NetworkStatus",
    fields={
        "node_id": GraphQLField(GraphQLNonNull(GraphQLString)),
        "peers_count": GraphQLField(GraphQLNonNull(GraphQLString)),
        "active_tasks": GraphQLField(GraphQLNonNull(GraphQLString)),
        "credits": GraphQLField(GraphQLNonNull(GraphQLString)),
        "reputation_score": GraphQLField(GraphQLNonNull(GraphQLString)),
    }
)

# –†–µ–∑–æ–ª–≤–µ—Ä—ã
def resolve_tasks(root, info: GraphQLResolveInfo, **args):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–¥–∞—á"""
    try:
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–¥–∞—á
        return []
    except Exception as e:
        raise Exception(f"Error fetching tasks: {e}")

def resolve_task(root, info: GraphQLResolveInfo, **args):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏"""
    task_id = args.get("task_id")
    try:
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
        return None
    except Exception as e:
        raise Exception(f"Error fetching task {task_id}: {e}")

def resolve_network_status(root, info: GraphQLResolveInfo, **args):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ç–∏"""
    try:
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ç–∏
        return {}
    except Exception as e:
        raise Exception(f"Error fetching network status: {e}")

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–µ–º
schema = GraphQLSchema(
    query=GraphQLObjectType(
        name="Query",
        fields={
            "tasks": GraphQLField(
                GraphQLList(TaskType),
                resolve=resolve_tasks
            ),
            "task": GraphQLField(
                TaskType,
                args={"task_id": GraphQLArgument(GraphQLNonNull(GraphQLString))},
                resolve=resolve_task
            ),
            "networkStatus": GraphQLField(
                NetworkStatusType,
                resolve=resolve_network_status
            )
        }
    ),
    mutation=GraphQLObjectType(
        name="Mutation",
        fields={
            "submitTask": GraphQLField(
                TaskType,
                args={
                    "task_type": GraphQLArgument(GraphQLNonNull(GraphQLString)),
                    "owner_id": GraphQLArgument(GraphQLNonNull(GraphQLString)),
                    "parameters": GraphQLArgument(GraphQLNonNull(GraphQLString)),
                },
                resolve=lambda root, info, **args: submit_task_mutation(root, info, **args)
            )
        }
    )
)

# –ú—É—Ç–∞—Ü–∏—è
def submit_task_mutation(root, info: GraphQLResolveInfo, **args):
    """–ü–æ–¥–∞—á–∞ –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ GraphQL"""
    try:
        task_data = {
            "task_type": args["task_type"],
            "owner_id": args["owner_id"],
            "parameters": json.loads(args["parameters"])
        }
        
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –ø–æ–¥–∞—á–∏ –∑–∞–¥–∞—á–∏
        return {"task_id": "generated_id", "status": "pending"}
        
    except Exception as e:
        raise Exception(f"Error submitting task: {e}")

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI()

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ GraphQL —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞
app.add_route("/graphql", GraphQLApp(schema=schema))
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö

#### 1. PostgreSQL –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

```python
import asyncpg
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import json
import time

@dataclass
class TaskRecord:
    """–ó–∞–ø–∏—Å—å –∑–∞–¥–∞—á–∏ –≤ –ë–î"""
    id: int
    task_id: str
    owner_id: str
    task_type: str
    status: str
    result: Optional[Dict[str, Any]]
    created_at: float
    updated_at: float
    execution_time: Optional[float] = None

class DatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool = None
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î"""
        self.pool = await asyncpg.create_pool(self.database_url)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
        async with self.pool.acquire() as conn:
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS tasks (
                    id SERIAL PRIMARY KEY,
                    task_id VARCHAR(255) UNIQUE,
                    owner_id VARCHAR(255),
                    task_type VARCHAR(50),
                    status VARCHAR(50),
                    result JSONB,
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW(),
                    execution_time FLOAT,
                    credits_used FLOAT DEFAULT 0.0
                )
            ''')
            
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS nodes (
                    id SERIAL PRIMARY KEY,
                    node_id VARCHAR(255) UNIQUE,
                    host VARCHAR(50),
                    port INTEGER,
                    capabilities JSONB,
                    reputation_score FLOAT DEFAULT 0.5,
                    last_seen TIMESTAMP DEFAULT NOW(),
                    credits_balance FLOAT DEFAULT 0.0
                )
            ''')
            
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS credit_transfers (
                    id SERIAL PRIMARY KEY,
                    from_node_id VARCHAR(255),
                    to_node_id VARCHAR(255),
                    amount FLOAT,
                    task_id VARCHAR(255),
                    timestamp TIMESTAMP DEFAULT NOW()
                )
            ''')
    
    async def save_task(self, task: TaskRecord):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –≤ –ë–î"""
        async with self.pool.acquire() as conn:
            await conn.execute('''
                INSERT INTO tasks (task_id, owner_id, task_type, status, result, execution_time)
                VALUES ($1, $2, $3, $4, $5, $6)
                ON CONFLICT (task_id) DO UPDATE SET
                    status = $4,
                    result = $5,
                    execution_time = $6,
                    updated_at = NOW()
            ''', task.task_id, task.owner_id, task.task_type, task.status, 
            json.dumps(task.result) if task.result else None, task.execution_time)
    
    async def get_task(self, task_id: str) -> Optional[TaskRecord]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∏–∑ –ë–î"""
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow('''
                SELECT * FROM tasks WHERE task_id = $1
            ''', task_id)
            
            if row:
                return TaskRecord(
                    id=row['id'],
                    task_id=row['task_id'],
                    owner_id=row['owner_id'],
                    task_type=row['task_type'],
                    status=row['status'],
                    result=row['result'],
                    created_at=row['created_at'].timestamp(),
                    updated_at=row['updated_at'].timestamp(),
                    execution_time=row['execution_time']
                )
            
            return None
    
    async def get_tasks_by_owner(self, owner_id: str, limit: int = 100) -> List[TaskRecord]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–¥–∞—á –≤–ª–∞–¥–µ–ª—å—Ü–∞"""
        async with self.pool.acquire() as conn:
            rows = await conn.fetch('''
                SELECT * FROM tasks WHERE owner_id = $1
                ORDER BY created_at DESC
                LIMIT $2
            ''', owner_id, limit)
            
            tasks = []
            for row in rows:
                task = TaskRecord(
                    id=row['id'],
                    task_id=row['task_id'],
                    owner_id=row['owner_id'],
                    task_type=row['task_type'],
                    status=row['status'],
                    result=row['result'],
                    created_at=row['created_at'].timestamp(),
                    updated_at=row['updated_at'].timestamp(),
                    execution_time=row['execution_time']
                )
                tasks.append(task)
            
            return tasks
    
    async def save_node(self, node_id: str, host: str, port: int, capabilities: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–∑–ª–∞ –≤ –ë–î"""
        async with self.pool.acquire() as conn:
            await conn.execute('''
                INSERT INTO nodes (node_id, host, port, capabilities)
                VALUES ($1, $2, $3, $4)
                ON CONFLICT (node_id) DO UPDATE SET
                    host = $2,
                    port = $3,
                    capabilities = $4,
                    last_seen = NOW()
            ''', node_id, host, port, json.dumps(capabilities))
    
    async def get_node(self, node_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É–∑–ª–∞ –∏–∑ –ë–î"""
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow('''
                SELECT * FROM nodes WHERE node_id = $1
            ''', node_id)
            
            return dict(row) if row else None
    
    async def record_credit_transfer(self, from_node_id: str, to_node_id: str, amount: float, task_id: str):
        """–ó–∞–ø–∏—Å—å –ø–µ—Ä–µ–≤–æ–¥–∞ –∫—Ä–µ–¥–∏—Ç–æ–≤"""
        async with self.pool.acquire() as conn:
            await conn.execute('''
                INSERT INTO credit_transfers (from_node_id, to_node_id, amount, task_id)
                VALUES ($1, $2, $3, $4)
            ''', from_node_id, to_node_id, amount, task_id)
    
    async def get_credit_history(self, node_id: str, limit: int = 100) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∫—Ä–µ–¥–∏—Ç–æ–≤ —É–∑–ª–∞"""
        async with self.pool.acquire() as conn:
            rows = await conn.fetch('''
                SELECT * FROM credit_transfers
                WHERE from_node_id = $1 OR to_node_id = $1
                ORDER BY timestamp DESC
                LIMIT $2
            ''', node_id, limit)
            
            return [dict(row) for row in rows]
    
    async def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        async with self.pool.acquire() as conn:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–¥–∞—á
            task_stats = await conn.fetch('''
                SELECT 
                    COUNT(*) as total_tasks,
                    COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_tasks,
                    COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_tasks,
                    AVG(execution_time) as avg_execution_time
                FROM tasks
            ''')
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É–∑–ª–æ–≤
            node_stats = await conn.fetch('''
                SELECT 
                    COUNT(*) as total_nodes,
                    AVG(reputation_score) as avg_reputation,
                    SUM(credits_balance) as total_credits
                FROM nodes
            ''')
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—Ä–µ–¥–∏—Ç–æ–≤
            credit_stats = await conn.fetch('''
                SELECT 
                    COUNT(*) as total_transfers,
                    SUM(amount) as total_amount,
                    AVG(amount) as avg_amount
                FROM credit_transfers
            ''')
            
            return {
                "tasks": dict(task_stats[0]),
                "nodes": dict(node_stats[0]),
                "credits": dict(credit_stats[0])
            }
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"""
        if self.pool:
            await self.pool.close()
```

#### 2. Redis –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è

```python
import redis.asyncio as redis
import json
import time
from typing import Optional, Dict, Any

class RedisCache:
    """–ö—ç—à –Ω–∞ –æ—Å–Ω–æ–≤–µ Redis"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, db: int = 0):
        self.redis = redis.Redis(host=host, port=port, db=db, decode_responses=True)
        self.default_ttl = 3600  # 1 —á–∞—Å
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"""
        await self.redis.ping()
        print("‚úÖ Redis –ø–æ–¥–∫–ª—é—á–µ–Ω")
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫—ç—à"""
        if ttl is None:
            ttl = self.default_ttl
        
        value_json = json.dumps(value, default=str)
        await self.redis.setex(key, ttl, value_json)
    
    async def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞"""
        value_json = await self.redis.get(key)
        
        if value_json is None:
            return None
        
        return json.loads(value_json)
    
    async def delete(self, key: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞"""
        await self.redis.delete(key)
    
    async def exists(self, key: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∫–ª—é—á–∞"""
        return await self.redis.exists(key) > 0
    
    async def expire(self, key: str, ttl: int):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –∂–∏–∑–Ω–∏ –∫–ª—é—á–∞"""
        await self.redis.expire(key, ttl)
    
    async def ttl(self, key: str) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∂–∏–∑–Ω–∏ –∫–ª—é—á–∞"""
        return await self.redis.ttl(key)
    
    async def incr(self, key: str, amount: int = 1) -> int:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç –∑–Ω–∞—á–µ–Ω–∏—è"""
        return await self.redis.incr(key, amount)
    
    async def decr(self, key: str, amount: int = 1) -> int:
        """–î–µ–∫—Ä–µ–º–µ–Ω—Ç –∑–Ω–∞—á–µ–Ω–∏—è"""
        return await self.redis.decr(key, amount)
    
    async def hset(self, key: str, field: str, value: Any):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª—è –≤ —Ö—ç—à–µ"""
        value_json = json.dumps(value, default=str)
        await self.redis.hset(key, field, value_json)
    
    async def hget(self, key: str, field: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª—è –∏–∑ —Ö—ç—à–∞"""
        value_json = await self.redis.hget(key, field)
        
        if value_json is None:
            return None
        
        return json.loads(value_json)
    
    async def hgetall(self, key: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–ª–µ–π —Ö—ç—à–∞"""
        fields = await self.redis.hgetall(key)
        
        result = {}
        for field, value_json in fields.items():
            result[field] = json.loads(value_json)
        
        return result
    
    async def lpush(self, key: str, values: list):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—á–∞–ª–æ —Å–ø–∏—Å–∫–∞"""
        values_json = [json.dumps(value, default=str) for value in values]
        await self.redis.lpush(key, *values_json)
    
    async def rpop(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–∑ –∫–æ–Ω—Ü–∞ —Å–ø–∏—Å–∫–∞"""
        value_json = await self.redis.rpop(key)
        
        if value_json is None:
            return None
        
        return json.loads(value_json)
    
    async def llen(self, key: str) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Å–ø–∏—Å–∫–∞"""
        return await self.redis.llen(key)
    
    async def flushdb(self):
        """–û—á–∏—Å—Ç–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        await self.redis.flushdb()
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"""
        await self.redis.close()
```

### Webhook —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è

```python
import aiohttp
import asyncio
from typing import Dict, Any, Optional
from dataclasses import dataclass
import json
import time

@dataclass
class WebhookConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–µ–±—Ö—É–∫–∞"""
    url: str
    method: str = "POST"
    headers: Dict[str, str] = None
    timeout: int = 30
    retry_count: int = 3
    retry_delay: int = 5

class WebhookManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –≤–µ–±—Ö—É–∫–æ–≤"""
    
    def __init__(self, config: WebhookConfig):
        self.config = config
        self.session = None
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏"""
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
    
    async def send_notification(self, event_type: str, data: Dict[str, Any]) -> bool:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è"""
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ payload
            payload = {
                "event_type": event_type,
                "timestamp": time.time(),
                "data": data
            }
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            async with self.session.request(
                method=self.config.method,
                url=self.config.url,
                json=payload,
                headers=self.config.headers or {}
            ) as response:
                if response.status == 200:
                    print(f"‚úÖ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {event_type}")
                    return True
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {response.status}")
                    return False
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤–µ–±—Ö—É–∫–∞: {e}")
            return False
    
    async def send_task_notification(self, task_id: str, status: str, result: Optional[Dict] = None):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –∑–∞–¥–∞—á–µ"""
        data = {
            "task_id": task_id,
            "status": status,
            "result": result
        }
        
        await self.send_notification("task_update", data)
    
    async def send_network_notification(self, event_type: str, data: Dict):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ —Å–µ—Ç–∏"""
        await self.send_notification(f"network_{event_type}", data)
    
    async def send_credit_notification(self, from_node: str, to_node: str, amount: float, task_id: str):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –ø–µ—Ä–µ–≤–æ–¥–µ –∫—Ä–µ–¥–∏—Ç–æ–≤"""
        data = {
            "from_node": from_node,
            "to_node": to_node,
            "amount": amount,
            "task_id": task_id
        }
        
        await self.send_notification("credit_transfer", data)
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def webhook_example():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–µ–±—Ö—É–∫–∞
    config = WebhookConfig(
        url="https://example.com/webhooks",
        method="POST",
        headers={"Authorization": "Bearer secret_token"},
        timeout=10,
        retry_count=3
    )
    
    # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
    webhook_manager = WebhookManager(config)
    await webhook_manager.initialize()
    
    try:
        # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –∑–∞–¥–∞—á–µ
        await webhook_manager.send_task_notification(
            task_id="task_123",
            status="completed",
            result={"result": "success", "execution_time": 2.5}
        )
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ —Å–µ—Ç–∏
        await webhook_manager.send_network_notification(
            event_type="node_joined",
            data={"node_id": "node_456", "timestamp": time.time()}
        )
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –∫—Ä–µ–¥–∏—Ç–∞—Ö
        await webhook_manager.send_credit_notification(
            from_node="node_123",
            to_node="node_456",
            amount=10.5,
            task_id="task_789"
        )
        
    finally:
        await webhook_manager.close()
```

---

## üé® –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ

### –ü–ª–∞–≥–∏–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

#### 1. –°–∏—Å—Ç–µ–º–∞ –ø–ª–∞–≥–∏–Ω–æ–≤

```python
import asyncio
import importlib
import inspect
from typing import Dict, List, Any, Optional, Type
from abc import ABC, abstractmethod
from dataclasses import dataclass
import logging

@dataclass
class PluginConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
    name: str
    version: str
    enabled: bool = True
    config: Dict[str, Any] = None

class PluginInterface(ABC):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–ª–∞–≥–∏–Ω–∞"""
    
    @abstractmethod
    async def initialize(self, config: Dict[str, Any]) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        pass
    
    @abstractmethod
    async def execute(self, data: Any) -> Any:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–ª–∞–≥–∏–Ω–∞"""
        pass
    
    @abstractmethod
    async def cleanup(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        """–ò–º—è –ø–ª–∞–≥–∏–Ω–∞"""
        pass
    
    @property
    @abstractmethod
    def version(self) -> str:
        """–í–µ—Ä—Å–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        pass

class PluginManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–ª–∞–≥–∏–Ω–æ–≤"""
    
    def __init__(self):
        self.plugins: Dict[str, PluginInterface] = {}
        self.plugin_configs: Dict[str, PluginConfig] = {}
        self.logger = logging.getLogger(__name__)
    
    async def load_plugin(self, plugin_class: Type[PluginInterface], config: PluginConfig) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–ª–∞–≥–∏–Ω–∞"""
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –ø–ª–∞–≥–∏–Ω–∞
            plugin_instance = plugin_class()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            await plugin_instance.initialize(config.config or {})
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self.plugins[config.name] = plugin_instance
            self.plugin_configs[config.name] = config
            
            self.logger.info(f"‚úÖ –ü–ª–∞–≥–∏–Ω {config.name} v{config.version} –∑–∞–≥—Ä—É–∂–µ–Ω")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–ª–∞–≥–∏–Ω–∞ {config.name}: {e}")
            return False
    
    async def unload_plugin(self, plugin_name: str) -> bool:
        """–í—ã–≥—Ä—É–∑–∫–∞ –ø–ª–∞–≥–∏–Ω–∞"""
        try:
            if plugin_name in self.plugins:
                # –û—á–∏—Å—Ç–∫–∞ –ø–ª–∞–≥–∏–Ω–∞
                await self.plugins[plugin_name].cleanup()
                
                # –£–¥–∞–ª–µ–Ω–∏–µ
                del self.plugins[plugin_name]
                del self.plugin_configs[plugin_name]
                
                self.logger.info(f"‚úÖ –ü–ª–∞–≥–∏–Ω {plugin_name} –≤—ã–≥—Ä—É–∂–µ–Ω")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –ø–ª–∞–≥–∏–Ω–∞ {plugin_name}: {e}")
            return False
    
    async def execute_plugin(self, plugin_name: str, data: Any) -> Any:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–ª–∞–≥–∏–Ω–∞"""
        try:
            if plugin_name not in self.plugins:
                raise ValueError(f"–ü–ª–∞–≥–∏–Ω {plugin_name} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            if not self.plugin_configs[plugin_name].enabled:
                raise ValueError(f"–ü–ª–∞–≥–∏–Ω {plugin_name} –æ—Ç–∫–ª—é—á–µ–Ω")
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            result = await self.plugins[plugin_name].execute(data)
            
            self.logger.debug(f"üîå –ü–ª–∞–≥–∏–Ω {plugin_name} –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–ª–∞–≥–∏–Ω–∞ {plugin_name}: {e}")
            raise
    
    def get_plugin_info(self, plugin_name: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–ª–∞–≥–∏–Ω–µ"""
        if plugin_name in self.plugins:
            return {
                "name": self.plugins[plugin_name].name,
                "version": self.plugins[plugin_name].version,
                "enabled": self.plugin_configs[plugin_name].enabled,
                "config": self.plugin_configs[plugin_name].config
            }
        return None
    
    def list_plugins(self) -> List[Dict[str, Any]]:
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–ª–∞–≥–∏–Ω–æ–≤"""
        plugins_info = []
        for name in self.plugins:
            info = self.get_plugin_info(name)
            if info:
                plugins_info.append(info)
        return plugins_info
    
    async def reload_plugin(self, plugin_name: str) -> bool:
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –ø–ª–∞–≥–∏–Ω–∞"""
        try:
            if plugin_name in self.plugins:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                config = self.plugin_configs[plugin_name]
                
                # –í—ã–≥—Ä—É–∂–∞–µ–º
                await self.unload_plugin(plugin_name)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–∞–Ω–æ–≤–æ
                return await self.load_plugin(self.plugins[plugin_name].__class__, config)
            
            return False
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –ø–ª–∞–≥–∏–Ω–∞ {plugin_name}: {e}")
            return False
```

#### 2. –ü—Ä–∏–º–µ—Ä –ø–ª–∞–≥–∏–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö

```python
import numpy as np
from typing import Dict, Any
from plugin_manager import PluginInterface, PluginConfig

class DataProcessorPlugin(PluginInterface):
    """–ü–ª–∞–≥–∏–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.config = None
    
    async def initialize(self, config: Dict[str, Any]) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        self.config = config
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.normalization_method = config.get('normalization_method', 'minmax')
        self.outlier_detection = config.get('outlier_detection', True)
        self.outlier_threshold = config.get('outlier_threshold', 3.0)
        
        print(f"üîß –ü–ª–∞–≥–∏–Ω DataProcessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def execute(self, data: Any) -> Any:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy array
            if isinstance(data, list):
                data_array = np.array(data)
            elif isinstance(data, np.ndarray):
                data_array = data
            else:
                raise ValueError("Unsupported data type")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            if self.normalization_method == 'minmax':
                data_array = self._minmax_normalize(data_array)
            elif self.normalization_method == 'zscore':
                data_array = self._zscore_normalize(data_array)
            
            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
            if self.outlier_detection:
                data_array = self._remove_outliers(data_array)
            
            return data_array.tolist()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    async def cleanup(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.config = None
        print("üßπ –ü–ª–∞–≥–∏–Ω DataProcessor –æ—á–∏—â–µ–Ω")
    
    @property
    def name(self) -> str:
        return "DataProcessor"
    
    @property
    def version(self) -> str:
        return "1.0.0"
    
    def _minmax_normalize(self, data: np.ndarray) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Min-Max"""
        min_val = np.min(data)
        max_val = np.max(data)
        
        if max_val == min_val:
            return np.zeros_like(data)
        
        return (data - min_val) / (max_val - min_val)
    
    def _zscore_normalize(self, data: np.ndarray) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Z-score"""
        mean = np.mean(data)
        std = np.std(data)
        
        if std == 0:
            return np.zeros_like(data)
        
        return (data - mean) / std
    
    def _remove_outliers(self, data: np.ndarray) -> np.ndarray:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤"""
        mean = np.mean(data)
        std = np.std(data)
        
        # –ì—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤
        lower_bound = mean - self.outlier_threshold * std
        upper_bound = mean + self.outlier_threshold * std
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        mask = (data >= lower_bound) & (data <= upper_bound)
        return data[mask]
```

#### 3. –ü—Ä–∏–º–µ—Ä –ø–ª–∞–≥–∏–Ω–∞ –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏

```python
from typing import Dict, Any, Optional
from plugin_manager import PluginInterface, PluginConfig
import hashlib
import time

class AuthPlugin(PluginInterface):
    """–ü–ª–∞–≥–∏–Ω –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.users = {}
        self.sessions = {}
    
    async def initialize(self, config: Dict[str, Any]) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        self.config = config
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        for user_data in config.get('users', []):
            self._create_user(user_data['username'], user_data['password'])
        
        print(f"üîê –ü–ª–∞–≥–∏–Ω Auth –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def execute(self, data: Any) -> Any:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        try:
            if isinstance(data, dict):
                action = data.get('action')
                
                if action == 'login':
                    return await self._login(data['username'], data['password'])
                elif action == 'logout':
                    return await self._logout(data['session_id'])
                elif action == 'validate':
                    return await self._validate_session(data['session_id'])
                else:
                    raise ValueError("Unknown action")
            
            raise ValueError("Invalid data format")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            raise
    
    async def cleanup(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.users.clear()
        self.sessions.clear()
        print("üßπ –ü–ª–∞–≥–∏–Ω Auth –æ—á–∏—â–µ–Ω")
    
    @property
    def name(self) -> str:
        return "Auth"
    
    @property
    def version(self) -> str:
        return "1.0.0"
    
    def _create_user(self, username: str, password: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        hashed_password = hashlib.sha256(password.encode()).hexdigest()
        self.users[username] = {
            'password': hashed_password,
            'created_at': time.time()
        }
    
    async def _login(self, username: str, password: str) -> Dict[str, Any]:
        """–õ–æ–≥–∏–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if username not in self.users:
            raise ValueError("User not found")
        
        hashed_password = hashlib.sha256(password.encode()).hexdigest()
        
        if self.users[username]['password'] != hashed_password:
            raise ValueError("Invalid password")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏
        session_id = hashlib.sha256(f"{username}{time.time()}".encode()).hexdigest()[:32]
        self.sessions[session_id] = {
            'username': username,
            'created_at': time.time(),
            'last_activity': time.time()
        }
        
        return {
            'session_id': session_id,
            'username': username,
            'expires_at': time.time() + 3600  # 1 —á–∞—Å
        }
    
    async def _logout(self, session_id: str) -> bool:
        """–í—ã—Ö–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if session_id in self.sessions:
            del self.sessions[session_id]
            return True
        return False
    
    async def _validate_session(self, session_id: str) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏"""
        if session_id not in self.sessions:
            raise ValueError("Invalid session")
        
        session = self.sessions[session_id]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –∂–∏–∑–Ω–∏
        if time.time() - session['last_activity'] > 3600:  # 1 —á–∞—Å
            del self.sessions[session_id]
            raise ValueError("Session expired")
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        session['last_activity'] = time.time()
        
        return {
            'valid': True,
            'username': session['username'],
            'expires_at': session['created_at'] + 3600
        }
```

### –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Ç–∏–ø—ã –∑–∞–¥–∞—á

#### 1. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∑–∞–¥–∞—á

```python
from core.task import Task, TaskType
from typing import Dict, Any, List
import time

class CustomTaskType:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á"""
    
    _custom_types = {}
    
    @classmethod
    def register_type(cls, task_type: str, handler_class: type):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏"""
        cls._custom_types[task_type] = handler_class
        print(f"üìù –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω —Ç–∏–ø –∑–∞–¥–∞—á–∏: {task_type}")
    
    @classmethod
    def get_handler(cls, task_type: str):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏"""
        return cls._custom_types.get(task_type)

class DataAnalysisTask(Task):
    """–ó–∞–¥–∞—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, owner_id: str, data: List, analysis_type: str, **kwargs):
        super().__init__(owner_id, TaskType.CUSTOM, **kwargs)
        self.data = data
        self.analysis_type = analysis_type
    
    def validate(self) -> List[str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–¥–∞—á–∏"""
        errors = []
        
        if not isinstance(self.data, list):
            errors.append("Data must be a list")
        
        if not self.analysis_type:
            errors.append("Analysis type is required")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞
        if self.analysis_type == "statistical":
            if len(self.data) < 2:
                errors.append("Statistical analysis requires at least 2 data points")
        
        return errors
    
    def execute(self) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞"""
        if self.analysis_type == "statistical":
            return self._statistical_analysis()
        elif self.analysis_type == "correlation":
            return self._correlation_analysis()
        else:
            raise ValueError(f"Unknown analysis type: {self.analysis_type}")
    
    def _statistical_analysis(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        import numpy as np
        
        data_array = np.array(self.data)
        
        return {
            'mean': float(np.mean(data_array)),
            'median': float(np.median(data_array)),
            'std': float(np.std(data_array)),
            'min': float(np.min(data_array)),
            'max': float(np.max(data_array)),
            'count': len(data_array),
            'timestamp': time.time()
        }
    
    def _correlation_analysis(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"""
        import numpy as np
        from scipy import stats
        
        if len(self.data) < 2:
            raise ValueError("Correlation analysis requires at least 2 datasets")
        
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [x1, x2, x3, ...]
        x = self.data[0]
        y = self.data[1]
        
        correlation, p_value = stats.pearsonr(x, y)
        
        return {
            'correlation': float(correlation),
            'p_value': float(p_value),
            'significant': p_value < 0.05,
            'timestamp': time.time()
        }

class ImageProcessingTask(Task):
    """–ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, owner_id: str, image_path: str, operation: str, **kwargs):
        super().__init__(owner_id, TaskType.CUSTOM, **kwargs)
        self.image_path = image_path
        self.operation = operation
    
    def validate(self) -> List[str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–¥–∞—á–∏"""
        errors = []
        
        if not self.image_path:
            errors.append("Image path is required")
        
        if not self.operation:
            errors.append("Operation is required")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        supported_operations = ["resize", "crop", "filter", "enhance"]
        if self.operation not in supported_operations:
            errors.append(f"Unsupported operation: {self.operation}")
        
        return errors
    
    def execute(self) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            from PIL import Image
            
            with Image.open(self.image_path) as img:
                if self.operation == "resize":
                    return self._resize_image(img)
                elif self.operation == "crop":
                    return self._crop_image(img)
                elif self.operation == "filter":
                    return self._apply_filter(img)
                elif self.operation == "enhance":
                    return self._enhance_image(img)
                else:
                    raise ValueError(f"Unknown operation: {self.operation}")
        
        except Exception as e:
            raise RuntimeError(f"Image processing failed: {e}")
    
    def _resize_image(self, img) -> Dict[str, Any]:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        # –ü—Ä–∏–º–µ—Ä: –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–æ 800x600
        resized = img.resize((800, 600))
        
        return {
            'operation': 'resize',
            'original_size': img.size,
            'new_size': resized.size,
            'timestamp': time.time()
        }
    
    def _crop_image(self, img) -> Dict[str, Any]:
        """–û–±—Ä–µ–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        # –ü—Ä–∏–º–µ—Ä: –æ–±—Ä–µ–∑–∫–∞ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–∏
        width, height = img.size
        left = width // 4
        top = height // 4
        right = 3 * width // 4
        bottom = 3 * height // 4
        
        cropped = img.crop((left, top, right, bottom))
        
        return {
            'operation': 'crop',
            'original_size': img.size,
            'new_size': cropped.size,
            'timestamp': time.time()
        }
    
    def _apply_filter(self, img) -> Dict[str, Any]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞"""
        # –ü—Ä–∏–º–µ—Ä: –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º—ã—Ç–∏—è
        from PIL import ImageFilter
        filtered = img.filter(ImageFilter.BLUR)
        
        return {
            'operation': 'filter',
            'filter_type': 'blur',
            'original_size': img.size,
            'new_size': filtered.size,
            'timestamp': time.time()
        }
    
    def _enhance_image(self, img) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        # –ü—Ä–∏–º–µ—Ä: —É–ª—É—á—à–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
        from PIL import ImageEnhance
        enhancer = ImageEnhance.Contrast(img)
        enhanced = enhancer.enhance(1.5)
        
        return {
            'operation': 'enhance',
            'enhancement_type': 'contrast',
            'factor': 1.5,
            'original_size': img.size,
            'new_size': enhanced.size,
            'timestamp': time.time()
        }

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á
CustomTaskType.register_type("data_analysis", DataAnalysisTask)
CustomTaskType.register_type("image_processing", ImageProcessingTask)
```

#### 2. –§–∞–±—Ä–∏–∫–∞ –∑–∞–¥–∞—á

```python
from typing import Dict, Any, Type, Optional
from core.task import Task, TaskType
from custom_tasks import DataAnalysisTask, ImageProcessingTask

class TaskFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á"""
    
    _task_classes: Dict[TaskType, Type[Task]] = {}
    _custom_task_classes: Dict[str, Type[Task]] = {}
    
    @classmethod
    def register_task_class(cls, task_type: TaskType, task_class: Type[Task]):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ –∑–∞–¥–∞—á–∏"""
        cls._task_classes[task_type] = task_class
        print(f"üìù –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –∫–ª–∞—Å—Å –∑–∞–¥–∞—á–∏: {task_type.value}")
    
    @classmethod
    def register_custom_task_class(cls, task_type: str, task_class: Type[Task]):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∑–∞–¥–∞—á–∏"""
        cls._custom_task_classes[task_type] = task_class
        print(f"üìù –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –∫–∞—Å—Ç–æ–º–Ω—ã–π –∫–ª–∞—Å—Å –∑–∞–¥–∞—á–∏: {task_type}")
    
    @classmethod
    def create_task(cls, task_type: str, owner_id: str, **kwargs) -> Task:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ç–∏–ø–æ–≤
        if task_type in cls._custom_task_classes:
            task_class = cls._custom_task_classes[task_type]
            return task_class(owner_id, **kwargs)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤
        try:
            enum_type = TaskType(task_type)
            if enum_type in cls._task_classes:
                task_class = cls._task_classes[enum_type]
                return task_class(owner_id, **kwargs)
        except ValueError:
            pass
        
        raise ValueError(f"Unknown task type: {task_type}")
    
    @classmethod
    def get_supported_types(cls) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á"""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–∏–ø—ã
        standard_types = [t.value for t in TaskType]
        
        # –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Ç–∏–ø—ã
        custom_types = list(cls._custom_task_classes.keys())
        
        return standard_types + custom_types

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á
TaskFactory.register_task_class(TaskType.RANGE_REDUCE, Task)
TaskFactory.register_task_class(TaskType.MAP, Task)
TaskFactory.register_task_class(TaskType.MAP_REDUCE, Task)
TaskFactory.register_task_class(TaskType.MATRIX_OPS, Task)
TaskFactory.register_task_class(TaskType.ML_INFERENCE, Task)
TaskFactory.register_task_class(TaskType.ML_TRAIN_STEP, Task)

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ç–∏–ø–æ–≤
TaskFactory.register_custom_task_class("data_analysis", DataAnalysisTask)
TaskFactory.register_custom_task_class("image_processing", ImageProcessingTask)
```

---

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞

### –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

#### 1. –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫

```python
import asyncio
import time
import psutil
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from collections import deque
import threading

@dataclass
class SystemMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
    timestamp: float
    cpu_percent: float
    memory_percent: float
    disk_percent: float
    network_io: Dict[str, float]
    process_count: int
    load_average: List[float]

@dataclass
class NetworkMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ —Å–µ—Ç–∏"""
    timestamp: float
    active_tasks: int
    pending_tasks: int
    completed_tasks: int
    failed_tasks: int
    total_nodes: int
    average_response_time: float
    throughput: float

@dataclass
class TaskMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–¥–∞—á"""
    timestamp: float
    task_type: str
    execution_time: float
    resource_usage: Dict[str, float]
    success_rate: float
    queue_length: int

class MetricsCollector:
    """–°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫"""
    
    def __init__(self, max_history: int = 1000):
        self.max_history = max_history
        self.system_metrics: deque = deque(maxlen=max_history)
        self.network_metrics: deque = deque(maxlen=max_history)
        self.task_metrics: deque = deque(maxlen=max_history)
        self.running = False
        self.collection_interval = 30  # 30 —Å–µ–∫—É–Ω–¥
        
    async def start(self):
        """–ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫"""
        self.running = True
        asyncio.create_task(self._collect_metrics())
        print("üìä –°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫ –∑–∞–ø—É—â–µ–Ω")
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫"""
        self.running = False
        print("üìä –°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    async def _collect_metrics(self):
        """–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫"""
        while self.running:
            try:
                # –°–±–æ—Ä —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                system_metrics = self._collect_system_metrics()
                self.system_metrics.append(system_metrics)
                
                # –°–±–æ—Ä —Å–µ—Ç–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
                network_metrics = self._collect_network_metrics()
                self.network_metrics.append(network_metrics)
                
                # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∑–∞–¥–∞—á
                task_metrics = self._collect_task_metrics()
                self.task_metrics.append(task_metrics)
                
                print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–±—Ä–∞–Ω—ã: {time.strftime('%H:%M:%S')}")
                
                await asyncio.sleep(self.collection_interval)
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫: {e}")
                await asyncio.sleep(60)
    
    def _collect_system_metrics(self) -> SystemMetrics:
        """–°–±–æ—Ä —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        # CPU
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # –ü–∞–º—è—Ç—å
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        # –î–∏—Å–∫
        disk = psutil.disk_usage('/')
        disk_percent = disk.percent
        
        # –°–µ—Ç—å
        network = psutil.net_io_counters()
        network_io = {
            'bytes_sent': network.bytes_sent,
            'bytes_recv': network.bytes_recv,
            'packets_sent': network.packets_sent,
            'packets_recv': network.packets_recv
        }
        
        # –ü—Ä–æ—Ü–µ—Å—Å—ã
